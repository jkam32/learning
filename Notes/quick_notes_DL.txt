Chap 1 Intro
------------
- Keras (backend using Theano / Tensorflow); by Francois Chollet
- mxnet (distributed; multi-machine learning)
- OpenCV
- scikit-image
- scikit-learn
- expected outcome of the book
	- load image
	- preprocess image (for training a CNN)
	- build own implementation of CNN
	- implement popular CNN architecture (AlexNet, VGGNet, GoogLeNet, ResNet, SqueezeNet)
	- etc

Chap 2 What is Deep Learning?
-----------------------------
- DL \subset ML \subset AI
	- AI: automatic machine reasoning
	- ML: pattern recognition and learning from data
		- Artificial Neural Network (ANN) is one algorithm that specializes in pattern recognition / data learning
	
- History of NN and DL
	- 1943 			- first NN by McCulloch and Pitts - binary classifer with manual tuning of weights on inputs 
	- 1950s 		- Perceptron algorithm by Rosenblatt - automatic learning of weights - the automatic procedure is a basis of Stochastic Gradient Descent (SGD)
	- 1969 			- publication by Minky and Papert - showed that Perceptron with a linear activation (regardless of depths) cannot solve a non-linear problem;
					- XOR dataset as an example (a non-linear problem); impossible to draw a straight line that separates the two types of objects
					- almost killed the research on NN
	- 1970s - 1980s - Werbos + Rumelhart + LeCun - backpropagation algorithm - enables the training of multi-layered feedforward NN
					- TODO: Draw an example of multi-layered feedforward NN
					- combined with non-linear activation functions can solve the XOR problem
					- the backprop algorithm allows the NN to learn from the mistakes
					- slow technology + lack of labelled training datasets cause NN computation of more than 2 hidden layers infeasible
					- technology advancement has made training of multiple hidden layers feasible
						- aka Deep Learning
						- multiple hidden layers enable hierachical learning
							- base layer learns simple concepts
							- higher layers learn the abstractions
						- prime example 
							- Convolutional Neural Network (CNN) by LeCun in recognizing hand-written digits
							- learns discrimating patterns (filters) by stacking layers on top of each other
							- base layers learns edges and cornes
							- higher layers build on top of base layers for discrimation of different digits

- Hierachical Feature Learning
	- ML algorithms fall under - (I) supervised (II) unsupervised (III) semi-supervised
	- ML and image classification  - identify patterns used to differentiate different images
	- Historically, hand-engineered features are used to quantify the contents of an image (i.e. raw pixels are not fed into the algorithm, 
		and feature extractions are first performed)
		- feature extractions are some black-box algorithms that output a vector of information that seeks to quantify the images
			- notable examples
				- Histogram of Oriented Gradient(HOG) + Linear SVM - detect objects where viewpoint angle of images did not vary much from trained images

	- Instead of hand-defining the processes, DL (or CNN) learns the features automatically.  

- How "Deep" is deep?
	- > 2 layers of NN
	- DL is always associated with "more data leads to higher accuracy" (compared to traidional ML techniques which accuracy pleateaus with the increase of available data points

Chap 3: Image Fundamentals
--------------------------

- Pixels
	- building blocks of every image
	- can be represented on 
		- grayscale - [0, 255]
		- color using RGB scale, where each color space is on a scale of [0, 255]
			- RGB is an example of an additive color space (the more you add of each color, it tends towards bright colors)
	- denoted in the unit of x pixels wide, y pixels tall for a dimension of (x, y)
	- we always preprocess the image via mean substraction or scaling
	- the origin always starts at the top left, instead of the conventional bottom left
	- when accessing the coordinate of a pixel, probably easier to think in terms of row first (in terms of y) then followed by column (in terms of x)
	- in openCV, watch out that the color is stored in the reverse order of BGR (rather than RGB) due to historial reasons
	- scaling and aspect ratios
		- aspect ratio := width / height
		- scaling without maintaining the aspect ratios can always lead to distortion of images
		- image classification algorithms always assume fixed-size input (in the form of (z X z, where z = 32, 64, 224, 227, 256, 299 among popular choices))
		- options include ignoring ratios and resize, resize along shortest dimension & center crop etc
		- no best method of scaling/ resizing

Chap 4: Image Classification Basics
-----------------------------------

- What is image classification?
	- Let there be an image I of W x H x 3 = N pixel, where W = width, H = height and 3 = depth of the color space (RGB in this case), and let there be categories of {A, B, C}. Image classification involves taking the image I and outputs a label 'A', 'B' or 'C' or a vector of probabilities associated with the probability of I belonging to 'A', 'B' or 'C'.
	- NB that our dataset consists of a collection of data points (images)

- Semantic Gap
	- Humans can differentiate cat and dog easily, but computers see the images as a big matrix of numbers. The difference between how human and computer sees an image is known as semantic gap.

- Possible Challenges
	- viewpoint variation - no matter how an object 'A' is rotated, it is still object 'A'
	- scale variation - tall, grande, venti coffee mug, still a coffee mug!
	- deformation - Recall the Gumby characters 
	- occlusions (part of object hidden) - picture of dog versus picture of dog under a blanket
	- illumination - an object subjet to different lighting conditions
	- background clutter - Where's Wally?
	- intra-class variation - all the different chairs, are still considered chairs
	- The classifier must handle not just independently but the combination of the challenges aforementioned.
	- The success of an image classification job depends on the scope.
		- Too broad a scope (try to classify everything) can cause the project to fail easily.
		- A narrower/ more well-defined scope make the projects easier to manage.

- Types of Learning
	- Supervised
	- Unsupervised
	- Semi-supevised - only a subset of training data is labelled; we try to label the rest of the unlabelled data and utilize them as additional training data; need to tolerate a degree of inaccuracy
	
- Deep Learning Pipeline
	- Instead of writing a gazillion of if/else statements to differentiate images, we let the algorithms learn on their own.
	- Step 1 - Gather images and their labels (a finite set of categories)
				- Try to ensure each category is uniformly well-represented (i.e. each category has more or less same samples)	
				- If a category is overly represented - could lead to overfitting on that category.
				- There may be a need to deal with class imbalances.
	- Step 2 - Split dataset (Train / Test)
				- How about the hyperparameters? Set them using a validation set (which comes from training data and is a fake test set)
	- Step 3 - Train the network
	- Step 4 - Evaluate the network

- Traditional ML techniques on image classification involve feature extraction (raw pixels -> vector of numbers used for training). There is no need of feature extraction for a CNN.

- There is a chance that trained NN performs well on test set does not perform well on new unseen data. This is a matter of generalization and will be discussed further.

Chap 5: Datasets for Image Classification
-----------------------------------------

- MNIST: Modified National Institute of Standards and Technology - to identify handwritten digits
	- the "hello world" of machine learning

- Animals: Dogs, Cats and Panda
	- combination of Kaggle's Dogs and Cats dataset + Panda images from the internet

- CIFAR-10
	- 10 classes
	- substantially harder than MNIST to obtain decent accuracy score
	- regularly used to benchmark new CNN architecture
	
- SMILES
	- images of smiling and not smiling

- Kaggles: Dogs vs Cats
	- 25,000 images of dogs and cats with varying resolution
	- how you preprocess the image can affect the accuracy of the classifiers

- Flowers-17
	- 80 images per class

- ImageNet (IMLSVRC), Stanford Cars, Facial Recognition, Adience, CALTECH-101, Indoor CPVR

- Rule of thumb: 1000 - 5000 images per class when training a neural network

Chap 6: Configuring your development environment
------------------------------------------------
- touches on setting up the environment locally or on cloud

Chap 7: First Image Classifer
-----------------------------
- Working with images
	- always be aware of the dataset size; does it fit in the RAM?
	
- kNN
	- hyperparameters tuning 
		- `k`
		- distance metrics (Euclidean Norm / Manhanttan metric etc) 
	- for each test record, find the closest k points, assign the test record to the label where majority of the k points are;
		
- codes in running kNN  
	- modules used	
		-sklearn
		- [OWN]: pyimagesearch
		- imutils
		- argparse
	- TODO: select the hyperparameter `k`; in the example, k=1 was used

- Some pros and cons of kNN:
	- simple to train, but slow in testing
		- but classifying a NEW testing point requires comparison to every single training point (to find the nearest k samples to make a classifying decision)
		- classifcation time for new sample is o(N)
		- always needs to store a replica of data somewhere (in-memory, as pointers to disk and so on ...)
			- big troubles when the data is in terms of Gigabytes, Terabytes
		- can mitigate the time cost by algorithms such as Approximate Nearest Neighbor (ANN) algorithms
	- more suited for low-dimensional spaces (images are high dimensional)
	- it is an algorithm that does not learn anything
	- gives us a baseline performance to benchmark against those of NN and CNN
	
Chap 8: Parametrized Learning
-----------------------------

- instead of spending a large chunk of time in classifying each new record, we instead spend more time in training the datasets
	- define the machine learning model to a number of parameters; regardless of the training size

- TODO: def'n of parametrized learning (Russell and Norvig; 2009)

- intro to linear classification
	- reference: Anrej Kaparthy CS231n (Stanford)
	- parametrization: 'process of defining the necessary parameters of a given model)
	- 4 components of parametrized learning
		- data 
			- images (RGB pixels, extracted features (via algorithms ...), labels)
		- scoring function
			- defined by a function f
				- input -> f(input) -> output class label
		- loss function
			- difference between predicted class labels and ground truth labels
			- higher accuracy -> lower loss and vice-versa
		- weights and biases (W and b)
			- parameters of the classifer
			- find W and b via some optimization methods that minimize the loss function
	- once we learned our `W` and `b` via optimization, classifying new test data is fast.
	- Linear Classification (from images to labels)
		-Training set x_i and associated label y_i
		- i = 1, 2, ..., N (N training samples) ; y_i \in {1, ..., K}, K is the number of category
		- Define a scoring function `f` that maps images to class label scores
			- f(x_i, W, b) = Wx_i + b, where dim(x_i) = (3072, 1), dim(W) = (3, 3072), dim(b) = (3, 1)
			- What are the variables that we can change? `W` and `b`
			- What is our goal? Use scoring function + loss function to optimize `W` and `b` to increase accuracy scores
		
	- Role of loss functions
		- The loss funcion quantifies how well our scoring function (`W` and `b` learned?) is doing
		- define: multi-class SVM loss
			- hinge loss function
			- squared hinge loss
		
			

		
	